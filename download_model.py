from transformers import AutoTokenizer, AutoModelForCausalLM

def download_gpt2xl():
    #model_name = "gpt2-xl"
    model_name = "gpt2-large"

    print(f"ğŸ”„ Downloading tokenizer for {model_name}...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    print(f"â¬‡ï¸  Downloading model weights for {model_name}...")
    model = AutoModelForCausalLM.from_pretrained(model_name)

    print("âœ… Download complete. Cached locally in HuggingFace cache directory.")
    print(f"ğŸ“‚ Location: ~/.cache/huggingface/ or set HF_HOME to customize")


if __name__ == "__main__":
    download_gpt2xl()

'''
â¬‡ï¸  Downloading model weights for gpt2-xl...
âœ… Download complete. Cached locally in HuggingFace cache directory.
ğŸ“‚ Location: ~/.cache/huggingface/ or set HF_HOME to customize
'''